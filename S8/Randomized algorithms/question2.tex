\section{Streaming and Dimensionality Reduction}
\subsection{Dimensionality reduction}
\begin{itemize}
	\item In dimensionality reduction the final goal is to represent a high dataset of high dimensionality in a lower dimensionality
  \begin{itemize}
  	\item To improve running times for algorithms using the dataset
  \end{itemize}
  \item Any $n$ point subset of the Euclidean space can be linearly embedded in $k=O(\lg n / e^2)$ dimensions without distorting the distances more than a factor of $(1 \pm \epsilon)$ for any $0 < \epsilon < \frac12$ 
  \item For many other types of dimensionality reductions (e.g. Principal Component Analysis) the original data points $x_1, \dots, x_n$ must be inherently low dimensional
  \begin{itemize}
  	\item No assumption of the input data is required by the Johnson-Lindenstrauss theorem
    \item This is the most efficient way to do distance preserving dimensionality reduction for the general case
  \end{itemize}
\end{itemize}

\subsection{The Johnson-Lindenstrauss Theorem}
\begin{itemize}
  \item \textbf{The Johnson-Lindenstrauss Theorem} For any $0<\epsilon < \frac12$ and any integer $n$, then for integer
  \[
    k = O(\frac{1}{\epsilon^2} \lg n)
  \]
  large enough and any $n$ points $x_1, \dots, x_n \in \mathbb R^d$	there exists a linear map $L: \mathbb R^d \to \mathbb R^k$ such that for any $1 \leq i$, $j \leq n$
\[
	(1-\epsilon) \| x_i - x_j \|^2_2 \leq \|Lx_i - Lx_j \|^2_2 \leq (1+\epsilon)\|x_i - x_j\| ^2_ 2
\]
Here the linear transformation $L=\frac{1}{\sqrt{k}} A$, where $A$ is a matrix with entries $a_{i,j} \sim \mathcal N(0,1)$ independently sampled
  \item \textbf{Lemma 1.} Fix any vector unit vector. For any $0 < \epsilon$, $\delta < \frac12$. For $k = O(\epsilon^{-2} \log \frac1\delta)$ large enough let $L = \frac{1}{\sqrt k} A$ by a random variable, where $A$ is a $k \times d$ random matrix whose entries are independent zero mean Gaussians $(\sim \mathcal N(0,1))$ Then:
\[
	\text{Pr}_{L}\left(\|L x\|^{2}-1 |>\varepsilon\right) \leq \delta
\]
  i.e. with probability $1-\delta$ of the norm of $x$ is distorted by a factor at most $(1 \pm \epsilon)$ by $L$
  \begin{itemize}
  	\item This generalizes to any non-unit vector $v$, since a vector $v$ can be wriiten as $v = \norm{v} = \frac{v}{\norm{v}}$ and the embedding is linear $Lv = \norm{v}L \frac{v}{\norm{v}}$ the following holds with probability $1-\delta$
    \[
      \norm{Lv}^2 = \norm{\norm{v} L \frac{v}{\norm{v}}} = \norm{v}^2 \norm{L \frac{v}{\norm{v}}}^2 \in \norm{v}^2 (1 \pm \epsilon)
    \]
  \end{itemize}
  \item \textit{Proof of The Johnson-Lindenstrauss Theorem} 
  \begin{itemize}
    \item A point set $x_1, \dots, x_n$ is fixed
    \item Set $\delta = \frac{1}{n^3}$ and $k=O(\epsilon^{-2} \log \frac{1}{\delta})$ large enough 
    \item Consider the random variable matrix $L = \frac{1}{\sqrt{k}}A$ where $a_{i,jk} \sim \mathcal N(0,1)$
    \item Observe that $\norm{L x_i - L x_j} = \norm{L (x_i - x_j)}$ by linearity
    \item Thus by \textbf{Lemma 1.} the probability that $L$ distorts the norm by more than $\epsilon$ is at most $\delta$
    \item Define $B_{i,j}$ to be the bad event that the norm between the difference of points $x_i$ and $x_j$ is distorted by more than epsilon 
    \item Using the union bound and lemma 1 one gets the following
    \[
      \Pr_{L}(\exists(i,j) B_{i,j} \text{ occurs }) \leq \sum_{i,j = 1, i<j}^n \Pr_L(B_{i,j}) = {n \choose 2} \frac{1}{n^3} \leq \frac{1}{n}
    \]
    \item Thus it holds with probability at least $1- \frac1m > 0$ for $n > 1$ that the matrix $L$ maintains all the pairwise norms within a factor $1 \pm \epsilon$
  \end{itemize}
\end{itemize}

\subsection{Improving Johnson-Lindenstrauss Transform}
\begin{itemize}
  \item The running time for the standard Johnson-Lindenstrauss Transform is $O(\frac{1}{\epsilon ^2}\lg{n}d)$ which can be improved 
	\item An approach to speed up JL is to use an embedding matrix $A$ that is \textbf{sparse} i.e. a matrix with only $t$ non-zero entries per column
  \begin{itemize}
    \item Improves the embedding time to $O(dt)$
    \item Is $O(\|x\|_0 t)$ where $\| x \|_0$ denotes the number of non-zeroes in $x$
    \item For a matrix $t^{-1/2}A$ pick a uniform set of $t = O(\epsilon^{-1} \lg n)$ rows adn assign the corresponding entries to either $-1$ or $1$ uniformly at random and independently
    \item The embedding time compared to standard JL goes from $O(\| x \|_0 \epsilon^{-2} \lg n)$ to $O(\| x \|_0 \epsilon^{-1} \lg n)$, that is an $\epsilon^{-1}$ improvement
  \end{itemize}
  \item In feature hashing one uses a sparse matrix with $t=1$
  \begin{itemize}
  	\item Preserves distances if the ratio 
    \[
      \frac{\norm{v}_\infty}{\norm{v}_2}
    \]
    is small
    \item Feature hashing into the optimal $k = O(\epsilon^{-2} \lg n)$ preserve all distances within $(1 \pm \epsilon)$ exactly when
  \end{itemize}
  \item Instead of using sparse matrices for embeddings, another approach is to use matrices for which there are efficient algorithms to compute the product $Ax$ (\textbf{Fast Johnson-Lindenstrauss Transform})
  \begin{itemize}
    \item The observation is that if the data vectors have only small coordinates i.e. the ratio $\| x \|_{\infty} / \| x \|_2$ is small, then one can use very sparse matrices for the embedding
  	\item The main idea is to first multiply $x$ with a matrix which ensures that coordinates becomes small and then use a sparse matrix afterwards
  \end{itemize}
\end{itemize}

\newpage
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "ra-noter"
%%% End:
